# 5. Image Registration

This directory contains the software and hardware source code for the results of Section 5 of the EuroSys'26 paper: *RoPeerTo: A Datacenter-Scale Architecture for Peer-To-Peer DMA between GPUs and FPGAs*.

The following is a brief guide on compiling and running this specific experiment presented in the paper. Most experiments consists of two folders: `hw` (hardware) and `sw` (software), both of which are built using `make`.

Environment requirements & set-up
The following requirements must be met to run the experiments:

- Hardware:
    - AMD Alveo card. Experiments in the paper were conducted on an Alveo u55c. Other Coyote-supported platforms (e.g., u280, u250)  work as well.
    - AMD Instinct GPU. Experiments in the paper were conducted on a MI100. Other accelerator cards work (e.g., MI210) though results can vary.
    - For performance reasons, the FPGA and the GPU should be placed on the same NUMA node on multi-node systems. While RoPeerTo also works on multi-NUMA node systems, there is some performance degradation.

- Software/OS:
    - Linux >= 6 with DMABuff support and hugepages enabled
    - Vivado suite, with Vitis HLS, >= 2022.1 for synthesizing FPGA bitstreams
    - CMake >= 3.5, supporting C++17 standard
    - ROCm libraries. Experiments in the papers were conducted with ROCm 6.3.3. Important to note; in systems with more than one GPU, targetting other GPUs (i.e. gpu_id != 0), does not work in ROCm 6.3.3. If only ever targeting one (the first GPU), the following text can be ignored. This is a bug which was fixed during the development of the paper. Therefore, for ROCm 6.3.3, in addition it's necessary to apply the patch and re-install only the ROCR Runtime (but not the rest of the ROCm libraries). To apply the patch and re-install it, follow the steps below:
        1. Clone the ROCR Runtime, checking out the correct branch (matching the release of ROCm on the system): ```git clone -b release/rocm-rel-6.3.x https://github.com/ROCm/ROCR-Runtime.git```
        2. Apply the changes from the pull request: https://github.com/ROCm/ROCR-Runtime/pull/315 to the file `amd_gpu_agent.cpp`
        3. Install the run-time:
        ```bash
        mkdir build && cd build
        cmake -DCMAKE_INSTALL_PREFIX=</path/to/install/folder> -DCMAKE_C_COMPILER=/opt/rocm-6.3.3/llvm/bin/clang -DCMAKE_CXX_COMPILER=/opt/rocm-6.3.3/llvm/bin/clang++ -DCMAKE_BUILD_TYPE=Release -DBUILD_SHARED_LIBS=ON ..
        make
        make install        
        ```
        4. If the installation completed correctly, `</path/to/install/folder>/lib` should contain a shared library file called `libhsa-runtime64.so`. Add it to your `LD_LIBRARY_PATH` as follows:
        ```
        export LD_LIBRARY_PATH=</path/to/install/folder>/lib/:$LD_LIBRARY_PATH
        ```
Experiments in the paper were conducted on the [AMD-ETH Heteregenous Accelerated Compute Cluster (HACC)](https://github.com/fpgasystems/hacc/tree/main), a publicly available cluster for research in systems, computer architecture, and accelerated applications. Its hardware equipment includes various compute nodes (Alveo U55C, V80, U250, U280, Instinct GPU etc.) which are connected via a high-speed (100G) network. With an account, the hardware can be synthesised on any of the build servers (hacc-build-01 or hacc-build-02) and the experiments can be executed on any of the HACC Boxes (nodes with a GPU and an FPGA; experiments in the paper were conducted on hacc-box-03, though the cluster set-up and its accompanying hardware may change at any point in the future without prior notice.)

**N.B.:** On hacc-build-01/02, Vivado 2024.1 can be loaded using `module load vivado/2024.1`.

**N.B.:** On the HACC Boxes, ROCm 6.3.3. can be loaded using `module load rocm/6.3.3`

## Data
To reproduce the exact results in the paper, please download and install the datasets at: 

Data used in this publication were generated by the National Cancer Institute Clinical Proteomic Tumor Analysis Consortium (CPTAC).https://doi.org/10.7937/k9/tcia.2018.pat12tbs. Patient: C3N-00704, Study: Dec 10, 2000 NM PET 18 FDG SKULL T, CT: WB STND, PET: WB 3D AC)

alternatively, contact the authors and we will provide the used data privately.

As an alternative to obtain comparable results -- but using generated images -- we provide, in the volumes folder, the image_generator.py script. 
By using

```
python3 image_generator.py
```

the scripts collect the IM1.png provided, creates a 3D volume and applies deformation by using the Kornia computer vision library.



## OpenCV setup
To install OpenCV on your machine, use the following commands:
```
export OPENCV_DIR=$HOME/opencv_build/opencv/build

mkdir -p $HOME/opencv_build && cd $HOME/opencv_build
git clone https://github.com/opencv/opencv.git
git clone https://github.com/opencv/opencv_contrib.git
mkdir -p "$OPENCV_DIR" && cd "$OPENCV_DIR"

cmake -D CMAKE_BUILD_TYPE=Release \
  -D CMAKE_INSTALL_PREFIX=$HOME/local \
  -D INSTALL_C_EXAMPLES=ON \
  -D INSTALL_PYTHON_EXAMPLES=ON \
  -D OPENCV_GENERATE_PKGCONFIG=ON \
  -D BUILD_EXAMPLES=ON \
  -D OPENCV_EXTRA_MODULES_PATH=$HOME/opencv_build/opencv_contrib/modules \
  ..

make -j
make install
```

To add your OpenCV to the PATH through the .bashrc file, modify the .bashrc file as follows:
```
export PKG_CONFIG_PATH=$HOME/local/lib/pkgconfig:$PKG_CONFIG_PATH
export LD_LIBRARY_PATH=$HOME/local/lib:$LD_LIBRARY_PATH
export OPENCV_DIR=$HOME/opencv_build/opencv/build
```

Depending on the system, some sub-dependencies may be needed: 
```
sudo apt install build-essential cmake git libgtk-3-dev pkg-config libavcodec-dev libavformat-dev libswscale-dev libv4l-dev libxvidcore-dev libx264-dev openexr libatlas-base-dev libopenexr-dev libgstreamer-plugins-base1.0-dev libgstreamer1.0-dev python3-dev python3-numpy libtbb2 libtbb-dev libjpeg-dev libpng-dev libtiff-dev libdc1394-dev gfortran -y
```

*NOTE: It may be needed to checkout the correct versions. Please refer to this link for installing instructions: https://learnopencv.com/install-opencv-3-4-4-on-ubuntu-18-04/*


## Synthesis & compilation

#### Hardware synthesis
Hardware builds can take hours, depending on the example complexity and synthesis flags. Therefore, if synthesizing on a remote node, it's recommended to ensure the process doesn't get terminated when the connection is lost, by using Linux utilities such as `screen` or `tmux`. To build the hardware, the following commands should be used:
```bash 
cd hw
mkdir -p build && cd build

# In the paper, we use the Alveo u55c; other Coyote-supported devices (u250, u280) work as well 
cmake .. -DFDEV_NAME=u55c \
  -DMI_HISTOTYPE=float \
  -DMI_IN_DIM=512 \
  -DMI_IN_BITS=8 \
  -DMI_PE_NUMBER=8 \
  -DMI_PE_ENTROPY=8 \
  -DMI_N_COUPLES_MAX=512 \
  -DMI_ENTR_ACC_SIZE=8 \
  -DMI_BIN_VAL=0 \
  -DMI_VITIS=ON \
  -DMI_CACHE_MEM=OFF \
  -DMI_USE_URAM=OFF


make project && make bitgen
```
Once complete, a bitstream can be found in: `hw/build/bitstreams/cyt_top.bit`. On the HACC, it is recommended to synthesize the hardware on one of the build nodes (hacc-build-01, hacc-build-02).

**N.B.:** Since hardware synthesis can take hours, we provide a pre-generated bitstream for the AMD Alveo u55c platform, found in `hw/bitstream/cyt_top.bit` (or one-sided).

The provided design targets the **U55C** platform, indentically to the one that can be synthesized by following the steps described below. It features:

- **8 Processing Elements (PEs)** for histogram computation  
- **8 Processing Elements (PEs)** for entropy computation  

Both stages follow a **map-reduce** computational approach.

For further details on the hardware design and the automation flow for different platforms and setups, please refer to the open-source mutual information reference repository:
https://github.com/necst/hephaestus

#### Driver compilation
The Coyote driver, which is required to interact with the FPGA, can be compiled with the following command:
```bash
cd <Ropeerto-root>/Coyote/driver/
make
```

Once complete, a driver module can be foind in `Coyote/driver/build/coyote_driver.ko`

**N.B.:** The driver must always be compiled on the node used for running experiments (e.g., hacc-box-03), as the build and experiment nodes may have different versions of Linux.

#### Loading Driver and Bitstream
According to Coyote Instruction: 
```bash
./<ropeerto-root>/Coyote/util/program_hacc_local.sh <path-to-bitstream> <path-to-cyt-coyote_driver.ko> <device_id>
```


#### Software compilation

The software compilation is largely similar to the hardware, but, typically much faster (typically completed within a minute), and varies based on what experiment to run.

```bash
cd sw 
```

**Image Registration**

To perform a complete image registration on hardware, with Coyote:
```
mkdir build && cd build
cmake .. -DEN_GPU=1 -DSRC=../image_registration.cpp 
make -j
./p2p_baseline <vfpga_id> <floating_path> <reference_path> <out_path> [<depth>] [<rangeX>] [<rangeY>] [<rangeANGZ>] [<runs>] [<gpu_id>]
```

To perform a complete image registration on hardware, with Coyote and P2P buffers:

```
mkdir build && cd build
cmake .. -DEN_GPU=1  -DSRC=../p2p_image_registration.cpp 
make -j
./p2p_baseline <vfpga_id> <floating_path> <reference_path> <out_path> [<depth>] [<rangeX>] [<rangeY>] [<rangeANGZ>] [<runs>] [<gpu_id>]
```

Parameters:

<ul>
  <li><em>vfpga_id</em>: id for the vFPGA (default device id: 0)</li>
  <li><em>floating_path</em>: path to the floating folder, used as floating volume</li>
  <li><em>reference_path</em>: path to the reference folder, used as reference volume</li>
  <li><em>out_path</em>: path to the OUTPUT folder</li>
  <li><em>depth</em>: number of 2D slices in the 3D image</li>
  <li><em>rangeX</em>: space of values to explore for TX, from –RangeX to +RangeX</li>
  <li><em>rangeY</em>: space of values to explore for TY, from –RangeY to +RangeY</li>
  <li><em>rangeANGZ</em>: space of values to explore for ANGZ, from –ANGZ to +ANGZ</li>
  <li><em>runs</em>: how many times to run the experiment</li>
  <li><em>gpu_id</em>: id of the GPU to use (default device id: 0)</li>
</ul>

Example:

```
./p2p_baseline -DEN_GPU=1  <VFPGA_ID> ../volumes/floating/ ../volumes/reference/ ./ 246 80 80 1 1
```


**Registration Step**

To evaluate one registration step with Coyote:

```
mkdir build && cd build
cmake .. -DEN_GPU=1  -DSRC=../registration_step.cpp
make -j
./p2p_baseline <vfpga_id> <floating_path> <reference_path> <out_path> <tx> <ty> <cos(ANG)> <runs> <gpu_id>
```

To evaluate one registration step with Coyote and P2P buffers:

```
mkdir build && cd build
cmake .. -DEN_GPU=1 -DSRC=../p2p_registration_step.cpp
make -j
./p2p_baseline <vfpga_id> <floating_path> <reference_path> <out_path> <tx> <ty> <cos(ANG)> <runs> <gpu_id>
```

To evaluate one registration step with Coyote, P2P buffers, and register programming for FPGA kernel launch:

```
mkdir build && cd build
cmake .. -DEN_GPU=1 -DSRC=../reg_prog_registration_step.cpp
make -j
./p2p_baseline <vfpga_id> <floating_path> <reference_path> <out_path> <tx> <ty> <cos(ANG)> <runs> <gpu_id>
```

Parameters:
<ul>
  <li><em>vfpga_id</em>: id for the vFPGA (default device id: 0)</li>
  <li><em>floating_path</em>: path to the floating folder, used as floating volume</li>
  <li><em>reference_path</em>: path to the reference folder, used as reference volume</li>
  <li><em>out_path</em>: path to the OUTPUT folder</li>
  <li><em>tx</em>: transformation along X axis</li>
  <li><em>ty</em>: transformation along Y axis</li>
  <li><em>cos(ANG)</em>: rotation of ANG along Z axis as cos(ANGZ) </li>
  <li><em>runs</em>: number of runs to perform </li>
  <li><em>gpu_id</em>: id of the GPU to use</li>
</ul>

Example:
```
mkdir output
./p2p_baseline <vfpga_id> ../volumes/floating/ ../volumes/reference/ output/ 10 10 0 1 1
```


**Transformation**

```
mkdir build && cd build
cmake .. -DSRC=../transformation.cpp
make -j
./p2p_baseline <floating_path> <reference_path> <out_path> <tx> <ty> <cos(ANG)> <gpu_id>
```

<ul>
  <li><em>floating_path</em>: path to the floating folder, used as floating volume</li>
  <li><em>reference_path</em>: path to the reference folder, used as reference volume</li>
  <li><em>out_path</em>: path to the OUTPUT folder</li>
  <li><em>tx</em>: transformation along X axis</li>
  <li><em>ty</em>: transformation along Y axis</li>
  <li><em>cos(ANG)</em>: rotation of ANG along Z axis as cos(ANGZ) </li>
  <li><em>gpu_id</em>: id of the GPU to use</li>
</ul>

Example:
```
mkdir output
./p2p_baseline ../volumes/floating/ ../volumes/reference/ output/ 10 10 0 1 1
```

**Mutual Information**

To evaluate mutual information with Coyote:

```
mkdir build && cd build
cmake .. -DSRC=../mutual_information.cpp
make -j
./p2p_baseline <vfpga_id> <floating_path> <reference_path>
```

<ul>
  <li><em>vpfga_id</em>: id for the vFPGA (default device id: 0)</li>
  <li><em>floating_path</em>: path to the floating folder, used as floating volume</li>
  <li><em>reference_path</em>: path to the reference folder, used as reference volume</li>
</ul>

Example:
```
./p2p_baseline <vfpga_id> ../volumes/floating/ ../volumes/reference/
```



# Register Programming

This repository demonstrates a register-programming approach for GPU–FPGA systems, with a focus on correctness in the presence of GPU caching mechanisms.

For a basic introduction to register programming, refer to the **Coyote tutorial – example `10_fpga_register_programming`**.

For image registration applications, refer to the guide describing the **register-programming-based registration step**.

---

## Main Idea

Register programming requires the GPU to write directly into FPGA registers. These writes are typically used to:

- transfer configuration or control data from the GPU to the FPGA;
- start an FPGA kernel execution (e.g., by writing `ap_start = 1`).

Modern GPUs rely on a hierarchical memory system with aggressive caching. In practice, multiple GPU writes are first accumulated in cache and then flushed to memory as entire cache lines.

While this behavior is beneficial for GPU-only workloads, it creates correctness issues when programming FPGA registers:

- the FPGA kernel may be started before all data has reached the FPGA;
- correct behavior may require FPGA design modifications to enforce coherence between GPU caches and FPGA registers.

---

## Fine-Grain Atomic Writes

To avoid constraining FPGA designs to GPU cache behavior while ensuring correct execution, **RoPeerTo** adopts a register-programming approach based on **fine-grain atomic writes**.

The following atomic store operation is used:

```c
#define STORE(DST, SRC) \
  __hip_atomic_store((DST), (SRC), __ATOMIC_SEQ_CST, __HIP_MEMORY_SCOPE_SYSTEM)
```

This operation enforces:

- fine-grained, non-cached writes;
- strict ordering between consecutive writes.

As a result, data writes to FPGA registers are guaranteed to complete before control signals such as kernel start commands.

Since atomic operations are significantly slower than cache-based memory accesses, they should be used **only for order-sensitive operations** (e.g., register programming and control signaling), and not for bulk data transfers.

---

## Hardware Support Notes

- **AMD MI100 and earlier GPUs** do not support true fine-grain memory operations. On these devices, atomic stores may still result in **coalesced and reordered writes**.
- **AMD MI210 GPUs and newer** fully support fine-grain memory accesses and are fully compatible with this programming model.

